Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at s10_33% and are newly initialized: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
wandb: Currently logged in as: zhpinkman. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /cluster/raid/home/zhivar.sourati/ExplagraphGen/wandb/run-20221123_220821-2p84ww0z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run max-margin
wandb: â­ï¸ View project at https://wandb.ai/zhpinkman/contrastive_data
wandb: ğŸš€ View run at https://wandb.ai/zhpinkman/contrastive_data/runs/2p84ww0z
Set SLURM handle signals.
/cluster/raid/home/zhivar.sourati/anaconda3/envs/explagraphgen/lib/python3.8/site-packages/transformers/generation_utils.py:760: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beam_id = beam_token_id // vocab_size
Set SLURM handle signals.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                 bs â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:              epoch â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            grad_mp â–
wandb:               loss â–ˆâ–…â–„â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–
wandb:         lr_group_0 â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:         lr_group_1 â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:                 mp â–
wandb:           n_params â–
wandb:       src_pad_frac â–…â–„â–…â–„â–…â–„â–„â–„â–…â–†â–„â–‡â–„â–„â–ƒâ–…â–ƒâ–„â–„â–„â–†â–‡â–†â–‡â–„â–‡â–†â–…â–†â–†â–…â–†â–‡â–„â–…â–…â–‚â–ˆâ–…â–
wandb:        src_pad_tok â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–ƒâ–†â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–…â–†â–†â–†â–„â–†â–…â–„â–„â–„â–„â–…â–‡â–ƒâ–„â–„â–‚â–ˆâ–…â–
wandb:         step_count â–â–‚â–„â–…â–‡â–ˆâ–ˆ
wandb:   test_avg_gen_len â–â–
wandb:  test_avg_gen_time â–â–
wandb:      test_avg_loss â–â–
wandb:    test_avg_rouge1 â–â–
wandb:    test_avg_rouge2 â–â–
wandb:    test_avg_rougeL â–â–
wandb: test_avg_rougeLsum â–â–
wandb:          test_loss â–
wandb:        test_rouge2 â–
wandb:                tpb â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–†â–‡â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–
wandb:    val_avg_gen_len â–â–†â–ˆâ–ˆâ–‡â–‡
wandb:   val_avg_gen_time â–â–„â–‡â–ˆâ–…â–…
wandb:       val_avg_loss â–â–‚â–…â–‡â–ˆâ–ˆ
wandb:     val_avg_rouge1 â–â–†â–…â–ˆâ–…â–…
wandb:     val_avg_rouge2 â–‚â–ˆâ–â–…â–…â–…
wandb:     val_avg_rougeL â–â–ˆâ–…â–†â–ƒâ–ƒ
wandb:  val_avg_rougeLsum â–â–ˆâ–…â–†â–„â–„
wandb:           val_loss â–
wandb:         val_rouge2 â–
wandb: 
wandb: Run summary:
wandb:                 bs 2
wandb:              epoch 4
wandb:        global_step 2808
wandb:            grad_mp 737.66861
wandb:               loss 0.76447
wandb:         lr_group_0 0.0
wandb:         lr_group_1 0.0
wandb:                 mp 737.66861
wandb:           n_params 737668608
wandb:       src_pad_frac 0.0102
wandb:        src_pad_tok 1
wandb:         step_count 7
wandb:   test_avg_gen_len 49.02
wandb:  test_avg_gen_time 0.19021
wandb:      test_avg_loss 0.56151
wandb:    test_avg_rouge1 51.22373
wandb:    test_avg_rouge2 28.09595
wandb:    test_avg_rougeL 41.59401
wandb: test_avg_rougeLsum 41.59632
wandb:          test_loss 0.56151
wandb:        test_rouge2 28.09595
wandb:                tpb 171
wandb:    val_avg_gen_len 49.2
wandb:   val_avg_gen_time 0.19033
wandb:       val_avg_loss 0.64248
wandb:     val_avg_rouge1 51.07554
wandb:     val_avg_rouge2 27.65275
wandb:     val_avg_rougeL 40.87601
wandb:  val_avg_rougeLsum 40.93056
wandb:           val_loss 0.64248
wandb:         val_rouge2 27.65275
wandb: 
wandb: Synced max-margin: https://wandb.ai/zhpinkman/contrastive_data/runs/2p84ww0z
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221123_220821-2p84ww0z/logs
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at s10_33% and are newly initialized: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
wandb: Currently logged in as: zhpinkman. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /cluster/raid/home/zhivar.sourati/ExplagraphGen/wandb/run-20221123_225828-mdjuxuye
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run contrastive
wandb: â­ï¸ View project at https://wandb.ai/zhpinkman/contrastive_data
wandb: ğŸš€ View run at https://wandb.ai/zhpinkman/contrastive_data/runs/mdjuxuye
Set SLURM handle signals.
/cluster/raid/home/zhivar.sourati/anaconda3/envs/explagraphgen/lib/python3.8/site-packages/transformers/generation_utils.py:760: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beam_id = beam_token_id // vocab_size
Set SLURM handle signals.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                 bs â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:              epoch â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            grad_mp â–
wandb:               loss â–ˆâ–…â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–â–
wandb:         lr_group_0 â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:         lr_group_1 â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:                 mp â–
wandb:           n_params â–
wandb:       src_pad_frac â–…â–„â–…â–„â–…â–„â–„â–„â–…â–†â–„â–‡â–„â–„â–ƒâ–…â–ƒâ–„â–„â–„â–†â–‡â–†â–‡â–„â–‡â–†â–…â–†â–†â–…â–†â–‡â–„â–…â–…â–‚â–ˆâ–…â–
wandb:        src_pad_tok â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–ƒâ–†â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–…â–†â–†â–†â–„â–†â–…â–„â–„â–„â–„â–…â–‡â–ƒâ–„â–„â–‚â–ˆâ–…â–
wandb:         step_count â–â–‚â–„â–…â–‡â–ˆâ–ˆ
wandb:   test_avg_gen_len â–â–
wandb:  test_avg_gen_time â–â–
wandb:      test_avg_loss â–â–
wandb:    test_avg_rouge1 â–â–
wandb:    test_avg_rouge2 â–â–
wandb:    test_avg_rougeL â–â–
wandb: test_avg_rougeLsum â–â–
wandb:          test_loss â–
wandb:        test_rouge2 â–
wandb:                tpb â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–†â–‡â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–
wandb:    val_avg_gen_len â–â–ˆâ–†â–…â–ˆâ–ˆ
wandb:   val_avg_gen_time â–â–ˆâ–†â–†â–ˆâ–ˆ
wandb:       val_avg_loss â–â–‚â–…â–‡â–ˆâ–ˆ
wandb:     val_avg_rouge1 â–â–†â–ˆâ–‡â–ˆâ–ˆ
wandb:     val_avg_rouge2 â–â–ˆâ–ˆâ–†â–ˆâ–ˆ
wandb:     val_avg_rougeL â–â–†â–ˆâ–ƒâ–…â–…
wandb:  val_avg_rougeLsum â–â–†â–ˆâ–ƒâ–…â–…
wandb:           val_loss â–
wandb:         val_rouge2 â–
wandb: 
wandb: Run summary:
wandb:                 bs 2
wandb:              epoch 4
wandb:        global_step 1404
wandb:            grad_mp 737.66861
wandb:               loss 0.27361
wandb:         lr_group_0 0.0
wandb:         lr_group_1 0.0
wandb:                 mp 737.66861
wandb:           n_params 737668608
wandb:       src_pad_frac 0.0102
wandb:        src_pad_tok 1
wandb:         step_count 7
wandb:   test_avg_gen_len 47.88
wandb:  test_avg_gen_time 0.18154
wandb:      test_avg_loss 0.54117
wandb:    test_avg_rouge1 51.27817
wandb:    test_avg_rouge2 27.98168
wandb:    test_avg_rougeL 41.13992
wandb: test_avg_rougeLsum 41.16379
wandb:          test_loss 0.54117
wandb:        test_rouge2 27.98168
wandb:                tpb 171
wandb:    val_avg_gen_len 51.46
wandb:   val_avg_gen_time 0.20273
wandb:       val_avg_loss 0.62677
wandb:     val_avg_rouge1 51.48916
wandb:     val_avg_rouge2 27.53166
wandb:     val_avg_rougeL 41.13067
wandb:  val_avg_rougeLsum 41.11173
wandb:           val_loss 0.62677
wandb:         val_rouge2 27.53166
wandb: 
wandb: Synced contrastive: https://wandb.ai/zhpinkman/contrastive_data/runs/mdjuxuye
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221123_225828-mdjuxuye/logs
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at s10_33% and are newly initialized: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
wandb: Currently logged in as: zhpinkman. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /cluster/raid/home/zhivar.sourati/ExplagraphGen/wandb/run-20221124_003856-861pp3bf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pos_aug
wandb: â­ï¸ View project at https://wandb.ai/zhpinkman/contrastive_data
wandb: ğŸš€ View run at https://wandb.ai/zhpinkman/contrastive_data/runs/861pp3bf
Set SLURM handle signals.
/cluster/raid/home/zhivar.sourati/anaconda3/envs/explagraphgen/lib/python3.8/site-packages/transformers/generation_utils.py:760: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beam_id = beam_token_id // vocab_size
Set SLURM handle signals.
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.078 MB uploaded (0.000 MB deduped)wandb: | 0.078 MB of 0.078 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                 bs â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              epoch â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–…
wandb:            grad_mp â–
wandb:               loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         lr_group_0 â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:         lr_group_1 â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:                 mp â–
wandb:           n_params â–
wandb:       src_pad_frac â–…â–„â–‡â–…â–„â–†â–‚â–ƒâ–‚â–â–„â–†â–ˆâ–ƒâ–ƒâ–†â–ƒâ–„â–…â–ƒâ–„â–‡â–„â–‡â–„â–ƒâ–â–ƒâ–†â–…â–„â–…â–‡â–…â–…â–„â–…â–…â–‚â–ƒ
wandb:        src_pad_tok â–„â–„â–‡â–„â–„â–†â–‚â–ƒâ–ƒâ–â–„â–…â–ˆâ–ƒâ–ƒâ–…â–ƒâ–ƒâ–„â–ƒâ–„â–‡â–„â–†â–„â–ƒâ–â–ƒâ–…â–„â–„â–„â–ˆâ–…â–„â–ƒâ–…â–†â–‚â–ƒ
wandb:         step_count â–â–‚â–„â–…â–‡â–ˆâ–ˆ
wandb:   test_avg_gen_len â–â–
wandb:  test_avg_gen_time â–â–
wandb:      test_avg_loss â–â–
wandb:    test_avg_rouge1 â–â–
wandb:    test_avg_rouge2 â–â–
wandb:    test_avg_rougeL â–â–
wandb: test_avg_rougeLsum â–â–
wandb:          test_loss â–
wandb:        test_rouge2 â–
wandb:                tpb â–‡â–†â–‡â–ƒâ–…â–‡â–„â–…â–‡â–„â–†â–‡â–ƒâ–â–†â–†â–…â–„â–‚â–‚â–„â–‚â–†â–ƒâ–„â–‡â–„â–…â–‡â–„â–‡â–‡â–†â–‡â–ƒâ–†â–ˆâ–„â–„â–„
wandb:    val_avg_gen_len â–ˆâ–â–†â–ƒâ–â–
wandb:   val_avg_gen_time â–ˆâ–‚â–ƒâ–ƒâ–â–
wandb:       val_avg_loss â–ˆâ–ƒâ–â–â–â–
wandb:     val_avg_rouge1 â–â–†â–ˆâ–…â–…â–…
wandb:     val_avg_rouge2 â–â–†â–ˆâ–‡â–†â–†
wandb:     val_avg_rougeL â–â–‡â–ˆâ–‡â–†â–†
wandb:  val_avg_rougeLsum â–â–‡â–ˆâ–‡â–†â–†
wandb:           val_loss â–
wandb:         val_rouge2 â–
wandb: 
wandb: Run summary:
wandb:                 bs 8
wandb:              epoch 4
wandb:        global_step 1776
wandb:            grad_mp 737.66861
wandb:               loss 0.78857
wandb:         lr_group_0 0.0
wandb:         lr_group_1 0.0
wandb:                 mp 737.66861
wandb:           n_params 737668608
wandb:       src_pad_frac 0.20455
wandb:        src_pad_tok 72
wandb:         step_count 7
wandb:   test_avg_gen_len 48.32
wandb:  test_avg_gen_time 0.18211
wandb:      test_avg_loss 0.81849
wandb:    test_avg_rouge1 51.96188
wandb:    test_avg_rouge2 28.3292
wandb:    test_avg_rougeL 41.43687
wandb: test_avg_rougeLsum 41.43034
wandb:          test_loss 0.81849
wandb:        test_rouge2 28.3292
wandb:                tpb 663
wandb:    val_avg_gen_len 46.96
wandb:   val_avg_gen_time 0.17883
wandb:       val_avg_loss 0.82102
wandb:     val_avg_rouge1 51.156
wandb:     val_avg_rouge2 27.76192
wandb:     val_avg_rougeL 40.80608
wandb:  val_avg_rougeLsum 40.84895
wandb:           val_loss 0.82102
wandb:         val_rouge2 27.76192
wandb: 
wandb: Synced pos_aug: https://wandb.ai/zhpinkman/contrastive_data/runs/861pp3bf
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221124_003856-861pp3bf/logs
