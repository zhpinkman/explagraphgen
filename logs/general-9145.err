Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at s10_33% and are newly initialized: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
wandb: Currently logged in as: zhpinkman. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /cluster/raid/home/zhivar.sourati/ExplagraphGen/wandb/run-20221126_040142-2896z8ia
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run simple
wandb: ⭐️ View project at https://wandb.ai/zhpinkman/contrastive_data
wandb: 🚀 View run at https://wandb.ai/zhpinkman/contrastive_data/runs/2896z8ia
Set SLURM handle signals.
/cluster/raid/home/zhivar.sourati/anaconda3/envs/explagraphgen/lib/python3.8/site-packages/transformers/generation_utils.py:760: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beam_id = beam_token_id // vocab_size
Set SLURM handle signals.
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.080 MB uploaded (0.000 MB deduped)wandb: | 0.080 MB of 0.080 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                 bs ███████████████████████████████████████▁
wandb:              epoch ▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆████████
wandb:        global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            grad_mp ▁
wandb:               loss █▆▅▅▄▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb:         lr_group_0 ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:         lr_group_1 ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:                 mp ▁
wandb:           n_params ▁
wandb:       src_pad_frac ▅▄▅▅▅▄▄▄▅▆▄▇▄▄▄▅▄▄▄▅▆▇▇▇▄▇▆▅▆▆▅▆█▄▅▅▂█▆▁
wandb:        src_pad_tok ▄▃▄▃▃▃▃▃▄▅▃▆▃▃▃▄▃▃▃▄▅▆▆▆▄▆▅▄▄▄▄▅▇▃▄▄▂█▅▁
wandb:         step_count ▁▂▄▅▇██
wandb:   test_avg_gen_len ▁▁
wandb:  test_avg_gen_time ▁▁
wandb:      test_avg_loss ▁▁
wandb:    test_avg_rouge1 ▁▁
wandb:    test_avg_rouge2 ▁▁
wandb:    test_avg_rougeL ▁▁
wandb: test_avg_rougeLsum ▁▁
wandb:          test_loss ▁
wandb:        test_rouge2 ▁
wandb:                tpb ▇▇▇▇▆▇▆▇▇▇▇▆▇▇▇▆▇█▇▇▇▇▇▇▇█▆▇▆▆▆▆▆▇▇▇▇▇▇▁
wandb:    val_avg_gen_len ▁▇▂█▅▅
wandb:   val_avg_gen_time ▁▇▄█▇▇
wandb:       val_avg_loss ▁▂▅▇██
wandb:     val_avg_rouge1 ▅▅▃█▁▁
wandb:     val_avg_rouge2 ▄█▄▆▁▁
wandb:     val_avg_rougeL ▁█▅▅▂▂
wandb:  val_avg_rougeLsum ▁█▆▅▂▂
wandb:           val_loss ▁
wandb:         val_rouge2 ▁
wandb: 
wandb: Run summary:
wandb:                 bs 2
wandb:              epoch 4
wandb:        global_step 5616
wandb:            grad_mp 737.66861
wandb:               loss 0.17623
wandb:         lr_group_0 0.0
wandb:         lr_group_1 0.0
wandb:                 mp 737.66861
wandb:           n_params 737668608
wandb:       src_pad_frac 0.0125
wandb:        src_pad_tok 1
wandb:         step_count 7
wandb:   test_avg_gen_len 50.38
wandb:  test_avg_gen_time 0.19057
wandb:      test_avg_loss 1.08776
wandb:    test_avg_rouge1 50.49619
wandb:    test_avg_rouge2 26.41169
wandb:    test_avg_rougeL 39.80651
wandb: test_avg_rougeLsum 39.7738
wandb:          test_loss 1.08776
wandb:        test_rouge2 26.41169
wandb:                tpb 153
wandb:    val_avg_gen_len 48.98
wandb:   val_avg_gen_time 0.18989
wandb:       val_avg_loss 1.11866
wandb:     val_avg_rouge1 50.30178
wandb:     val_avg_rouge2 25.83065
wandb:     val_avg_rougeL 39.23471
wandb:  val_avg_rougeLsum 39.2105
wandb:           val_loss 1.11866
wandb:         val_rouge2 25.83065
wandb: 
wandb: Synced simple: https://wandb.ai/zhpinkman/contrastive_data/runs/2896z8ia
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221126_040142-2896z8ia/logs
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at s10_33% and are newly initialized: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
wandb: Currently logged in as: zhpinkman. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /cluster/raid/home/zhivar.sourati/ExplagraphGen/wandb/run-20221126_044954-2tsfh5wr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pos_aug
wandb: ⭐️ View project at https://wandb.ai/zhpinkman/contrastive_data
wandb: 🚀 View run at https://wandb.ai/zhpinkman/contrastive_data/runs/2tsfh5wr
Set SLURM handle signals.
/cluster/raid/home/zhivar.sourati/anaconda3/envs/explagraphgen/lib/python3.8/site-packages/transformers/generation_utils.py:760: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beam_id = beam_token_id // vocab_size
Set SLURM handle signals.
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.079 MB of 0.079 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                 bs ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:              epoch ▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆█████████
wandb:        global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▅
wandb:            grad_mp ▁
wandb:               loss █▃▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         lr_group_0 ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:         lr_group_1 ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:                 mp ▁
wandb:           n_params ▁
wandb:       src_pad_frac ▅▄▇▅▄▆▂▃▂▁▄▆█▃▃▆▃▄▅▃▄▇▄▇▄▃▁▃▆▅▄▅▇▅▅▄▅▅▂▃
wandb:        src_pad_tok ▄▄▇▄▄▆▂▃▃▁▄▅█▃▃▅▃▃▄▃▄▇▄▆▄▃▁▃▅▄▄▄█▅▄▃▅▆▂▃
wandb:         step_count ▁▂▄▅▇██
wandb:   test_avg_gen_len ▁▁
wandb:  test_avg_gen_time ▁▁
wandb:      test_avg_loss ▁▁
wandb:    test_avg_rouge1 ▁▁
wandb:    test_avg_rouge2 ▁▁
wandb:    test_avg_rougeL ▁▁
wandb: test_avg_rougeLsum ▁▁
wandb:          test_loss ▁
wandb:        test_rouge2 ▁
wandb:                tpb ▇▆▇▃▅▇▄▅▇▄▆▇▃▁▆▆▅▄▂▂▄▂▆▃▄▇▄▅▇▄▇▇▆▇▃▆█▄▄▄
wandb:    val_avg_gen_len █▅▅▃▁▁
wandb:   val_avg_gen_time █▇▇▁▁▁
wandb:       val_avg_loss █▂▁▁▁▁
wandb:     val_avg_rouge1 ▁█▆▆▇▇
wandb:     val_avg_rouge2 ▁█▃▆▇▇
wandb:     val_avg_rougeL ▁█▅███
wandb:  val_avg_rougeLsum ▁█▅███
wandb:           val_loss ▁
wandb:         val_rouge2 ▁
wandb: 
wandb: Run summary:
wandb:                 bs 8
wandb:              epoch 4
wandb:        global_step 1776
wandb:            grad_mp 737.66861
wandb:               loss 0.73384
wandb:         lr_group_0 0.0
wandb:         lr_group_1 0.0
wandb:                 mp 737.66861
wandb:           n_params 737668608
wandb:       src_pad_frac 0.20455
wandb:        src_pad_tok 72
wandb:         step_count 7
wandb:   test_avg_gen_len 47.28
wandb:  test_avg_gen_time 0.17557
wandb:      test_avg_loss 0.9226
wandb:    test_avg_rouge1 51.76135
wandb:    test_avg_rouge2 27.02666
wandb:    test_avg_rougeL 40.21291
wandb: test_avg_rougeLsum 40.17314
wandb:          test_loss 0.9226
wandb:        test_rouge2 27.02666
wandb:                tpb 663
wandb:    val_avg_gen_len 46.7
wandb:   val_avg_gen_time 0.17449
wandb:       val_avg_loss 0.91855
wandb:     val_avg_rouge1 51.39409
wandb:     val_avg_rouge2 26.83174
wandb:     val_avg_rougeL 40.24757
wandb:  val_avg_rougeLsum 40.24474
wandb:           val_loss 0.91855
wandb:         val_rouge2 26.83174
wandb: 
wandb: Synced pos_aug: https://wandb.ai/zhpinkman/contrastive_data/runs/2tsfh5wr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221126_044954-2tsfh5wr/logs
